{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Advantage-Actor Critic (A2C) - 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel. \n",
    "\n",
    "Firstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscal, take max between frames, skip frames, stack them together, prepares for PyTorch and normalizes to [0, 1]) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\n",
    "File `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from atari_wrappers import nature_dqn_env\n",
    "\n",
    "nenvs = 8    # change this if you have more than 8 CPU ;)\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs)\n",
    "\n",
    "n_actions = env.action_space.spaces[0].n\n",
    "obs = env.reset()\n",
    "assert obs.shape == (nenvs, 4, 84, 84)\n",
    "assert obs.dtype == np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to implement a model that predicts logits of policy distribution and critic value. Use shared backbone. You may use same architecture as in DQN task with one modification: instead of having a single output layer, it must have two output layers taking as input the output of the last hidden layer (one for actor, one for critic). \n",
    "\n",
    "Still it may be very helpful to make more changes:\n",
    "* use orthogonal initialization with gain $\\sqrt{2}$ and initialize biases with zeros;\n",
    "* use more filters (e.g. 32-64-64 instead of 16-32-64);\n",
    "* use two-layer heads for actor and critic or add a linear layer into backbone;\n",
    "\n",
    "**Danger:** do not divide on 255, input is already normalized to [0, 1] in our wrappers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch.nn import Flatten\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        \n",
    "        \n",
    "        self.conv_layers = nn.Sequential(nn.Conv2d(4,32,(3,3),stride=2),nn.ReLU(),\n",
    "                                         nn.Conv2d(32,64,(3,3),stride=2), nn.ReLU(),\n",
    "                                         nn.Conv2d(64,64,(3,3),stride=2),nn.ReLU())\n",
    "        self.hidden = nn.Linear(5184,200)\n",
    "        self.linear = nn.Linear(200,self.n_actions)\n",
    "        self.linear_val = nn.Linear(200,1)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        for layer in self.conv_layers:\n",
    "            if isinstance(layer,nn.Conv2d):\n",
    "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "            \n",
    "        nn.init.orthogonal_(self.hidden.weight, gain=np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.linear.weight, gain=np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.linear_val.weight, gain=np.sqrt(2))\n",
    "        \n",
    "        nn.init.constant_(self.hidden.bias, 0)\n",
    "        nn.init.constant_(self.linear.bias, 0)\n",
    "        nn.init.constant_(self.linear_val.bias, 0)\n",
    "        \n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        conv = self.conv_layers(state_t)\n",
    "        hidden = self.hidden(Flatten()(conv))\n",
    "        probs = self.linear(hidden)\n",
    "        value = self.linear_val(hidden)\n",
    "        return self.softmax(probs), value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a **dictionary** of all the arrays that are needed to interact with an environment and train the model.\n",
    "\n",
    "**Important**: \"actions\" will be sent to environment, they must be numpy array or list, not PyTorch tensor.\n",
    "\n",
    "Note: you can add more keys, e.g. it can be convenient to compute entropy right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "from collections import defaultdict\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs):\n",
    "        '''\n",
    "        input:|\n",
    "            inputs - numpy array, (batch_size x channels x width x height)\n",
    "        output: dict containing keys ['actions', 'logits', 'log_probs', 'values']:\n",
    "            'actions' - selected actions, numpy, (batch_size)\n",
    "            'logits' - actions logits, tensor, (batch_size x num_actions)\n",
    "            'log_probs' - log probs of selected actions, tensor, (batch_size)\n",
    "            'values' - critic estimations, tensor, (batch_size)\n",
    "        '''\n",
    "        logits, values = self.model(torch.tensor(inputs))\n",
    "        actions = []\n",
    "        for row in logits:\n",
    "            probs =(row.detach()/row.sum().detach()).numpy()\n",
    "            actions.append(np.random.choice(range(6),p=probs))\n",
    "\n",
    "        actions = np.array(actions)\n",
    "\n",
    "        log_probs = torch.log(logits[list(range(len(logits))),actions])\n",
    "        \n",
    "        entropy = (logits*torch.log(logits)).sum(1)\n",
    "\n",
    "        return {\n",
    "            \"actions\": actions,\n",
    "            \"logits\": logits,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values.view(-1),\n",
    "            \"entropy\":entropy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will pass the environment and policy to a runner that collects rollouts from the environment. \n",
    "The class is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runners import EnvRunner\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
    "keys \n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'dones'\n",
    "* 'actions'\n",
    "* all other keys that you defined in `Policy`\n",
    "\n",
    "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory, or rollout length. Let's have a look at how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_actions)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10144\\2481146560.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(probs), value\n"
     ]
    }
   ],
   "source": [
    "# generates new rollout\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['actions', 'logits', 'log_probs', 'values', 'entropy', 'observations', 'rewards', 'dones'])\n"
     ]
    }
   ],
   "source": [
    "# what is inside\n",
    "print(trajectory.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each key the length of the corresponding list is the number of steps. The size of each items is the size of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert 'logits' in trajectory, \"Not found: policy didn't provide logits\"\n",
    "assert 'log_probs' in trajectory, \"Not found: policy didn't provide log_probs of selected actions\"\n",
    "assert 'values' in trajectory, \"Not found: policy didn't provide critic estimations\"\n",
    "assert trajectory['logits'][0].shape == (nenvs, n_actions), \"logits wrong shape\"\n",
    "assert trajectory['log_probs'][0].shape == (nenvs,), \"log_probs wrong shape\"\n",
    "assert trajectory['values'][0].shape == (nenvs,), \"values wrong shape\"\n",
    "\n",
    "for key in trajectory.keys():\n",
    "    assert len(trajectory[key]) == 5, \\\n",
    "    f\"something went wrong: 5 steps should have been done, got trajectory of length {len(trajectory[key])} for '{key}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with this trajectory a bit. To train the critic you will need to compute the value targets. It will also be used as an estimation of $Q$ for actor training.\n",
    "\n",
    "You should use all available rewards for value targets, so the formula for the value targets is simple:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "where $s_{t + T}$ is the latest observation of the environment.\n",
    "\n",
    "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
    "Thus, we can implement and use `ComputeValueTargets` callable. \n",
    "\n",
    "**Do not forget** to use `trajectory['dones']` flags to check if you need to add the value targets at the next step when \n",
    "computing value targets for the current step.\n",
    "\n",
    "**Bonus (+0.5 pts):** implement [Generalized Advantage Estimation (GAE)](https://arxiv.org/pdf/1506.02438.pdf) instead; use $\\lambda \\approx 0.95$ or even closer to 1 in experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, policy, gamma=0.99):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, trajectory, latest_observation):\n",
    "        '''\n",
    "        This method should modify trajectory inplace by adding \n",
    "        an item with key 'value_targets' to it\n",
    "        \n",
    "        input:\n",
    "            trajectory - dict from runner\n",
    "            latest_observation - last state, numpy, (num_envs x channels x width x height)\n",
    "        '''\n",
    "        last_value = self.policy.act(latest_observation)[\"values\"].detach().cpu().numpy()\n",
    "        value_targets = []\n",
    "        G_curr = last_value\n",
    "        T = len(trajectory[\"rewards\"])\n",
    "        for i in range(T-1,-1,-1):\n",
    "            G_curr = trajectory[\"rewards\"][i] + self.gamma * G_curr * (1-trajectory[\"dones\"][i])\n",
    "            value_targets.append(torch.tensor(G_curr))\n",
    "        value_targets.reverse()\n",
    "        trajectory['value_targets'] = value_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing value targets we will transform lists of interactions into tensors\n",
    "with the first dimension `batch_size` which is equal to `T * nenvs`.\n",
    "\n",
    "You need to make sure that after this transformation `\"log_probs\"`, `\"value_targets\"`, `\"values\"` are 1-dimensional PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
    "    def __call__(self, trajectory, latest_observation):\n",
    "        # Modify trajectory inplace. \n",
    "        trajectory[\"log_probs\"] = torch.stack(trajectory[\"log_probs\"]).view(-1)\n",
    "        trajectory[\"value_targets\"] = torch.stack(trajectory[\"value_targets\"]).view(-1)\n",
    "        trajectory[\"values\"] = torch.stack(trajectory[\"values\"]).view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do more sanity checks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10144\\2481146560.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(probs), value\n"
     ]
    }
   ],
   "source": [
    "runner = EnvRunner(env, policy, nsteps=5, transforms=[ComputeValueTargets(policy),\n",
    "                                                      MergeTimeBatch()])\n",
    "\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sanity checks\n",
    "assert 'value_targets' in trajectory, \"Value targets not found\"\n",
    "assert trajectory['log_probs'].shape == (5 * nenvs,)\n",
    "assert trajectory['value_targets'].shape == (5 * nenvs,)\n",
    "assert trajectory['values'].shape == (5 * nenvs,)\n",
    "\n",
    "assert trajectory['log_probs'].requires_grad, \"Gradients are not available for actor head!\"\n",
    "assert trajectory['values'].requires_grad, \"Gradients are not available for critic head!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2220, 0.2220, 0.2220, 0.2220, 0.2220, 0.2220, 0.2220, 0.2220, 0.2382,\n",
       "        0.2382, 0.2382, 0.2382, 0.2382, 0.2382, 0.2382, 0.2382, 0.2170, 0.2170,\n",
       "        0.2170, 0.2170, 0.2170, 0.2170, 0.2170, 0.2170, 0.2037, 0.2037, 0.2037,\n",
       "        0.2037, 0.2037, 0.2037, 0.2037, 0.2037, 0.2269, 0.2269, 0.2269, 0.2269,\n",
       "        0.2269, 0.2269, 0.2269, 0.2269])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory['values'].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to implement the advantage actor critic algorithm itself. You can look into [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and lectures ([part 1](https://www.youtube.com/watch?v=Ds1trXd6pos&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=5), [part 2](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=6)) by Sergey Levine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class A2C:\n",
    "    def __init__(self, policy, optimizer, value_loss_coef=0.25, entropy_coef=0.01, max_grad_norm=0.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "    \n",
    "    def loss(self, trajectory, write):\n",
    "        policy_loss = (trajectory['log_probs'] * (trajectory['values'].detach() - trajectory['value_targets'])).mean()\n",
    "        target_loss = ((trajectory['values'] - trajectory['value_targets'])**2).mean()\n",
    "        entropy_loss = torch.stack(trajectory[\"entropy\"]).view(-1).mean()\n",
    "\n",
    "        write('losses', {\n",
    "            'policy loss': policy_loss,\n",
    "            'critic_loss': target_loss,\n",
    "            'entropy_loss': entropy_loss\n",
    "        })\n",
    "        \n",
    "        # additional logs\n",
    "        write('critic/advantage', (trajectory[\"values\"] - trajectory[\"value_targets\"]).mean())\n",
    "        write('critic/values', {\n",
    "            'value predictions': trajectory[\"values\"].mean(),\n",
    "            'value targets': trajectory[\"value_targets\"].mean(),\n",
    "        })\n",
    "        \n",
    "\n",
    "        return policy_loss + target_loss * self.value_loss_coef +  entropy_loss * self.entropy_coef \n",
    "\n",
    "    def train(self, runner):\n",
    "        trajectory = runner.get_next()\n",
    "        loss = self.loss(trajectory,runner.write)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.policy.model.parameters(),self.max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        # use runner.write to log scalar to tensorboard\n",
    "#         runner.write('gradient norm', )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train your model. For optimization we suggest you use RMSProp with learning rate 7e-4 (you can also linearly decay it to 0), smoothing constant (alpha in PyTorch) equal to 0.99 and epsilon equal to 1e-5.\n",
    "\n",
    "We recommend to train for at least 10 million environment steps across all batched environments (takes ~3 hours on a single GTX1080 with 8 CPU). It should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last episodes in each environment in the batch) of about 600. **Your goal is to reach 500**.\n",
    "\n",
    "Notes:\n",
    "* if your reward is stuck at ~200 for more than 2M steps then probably there is a bug\n",
    "* if your gradient norm is >10 something probably went wrong\n",
    "* make sure your `entropy loss` is negative, your `critic loss` is positive\n",
    "* make sure you didn't forget `.detach` in losses where it's needed\n",
    "* `actor loss` should oscillate around zero or near it; do not expect loss to decrease in RL ;)\n",
    "* you can experiment with `nsteps` (\"rollout length\"); standard rollout length is 5 or 10. Note that this parameter influences how many algorithm iterations is required to train on 10M steps (or 40M frames --- we used frameskip in preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(6)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=10, transforms=[ComputeValueTargets(policy),\n",
    "                                                      MergeTimeBatch()])\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),lr=7e-4,eps=1e-5)\n",
    "\n",
    "a2c = A2C(policy,optimizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dct = torch.load(\"modA2C_30000\")\n",
    "# model.load_state_dict(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score():\n",
    "    env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, \n",
    "                     clip_reward=False, summaries=False, episodic_life=False)\n",
    "    sessions = evaluate(env, policy, n_games=30)\n",
    "    score = sessions.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, n_games=1, t_max=10000):\n",
    "    '''\n",
    "    Plays n_games and returns rewards\n",
    "    '''\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        \n",
    "        R = 0\n",
    "        for _ in range(t_max):\n",
    "            action = policy.act(np.array([s]))[\"actions\"][0]\n",
    "            \n",
    "            s, r, done, _ = env.step(action)\n",
    "            \n",
    "            R += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(4*10**4)):\n",
    "    a2c.train(runner)\n",
    "    if i % 10000 == 0:\n",
    "#         print(i,compute_score())\n",
    "        torch.save(model.state_dict(), f\"modA2C_{i}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, \n",
    "                     clip_reward=False, summaries=False, episodic_life=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score: 528.6666666666666\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# evaluation will take some time!\n",
    "sessions = evaluate(env, policy, n_games=30)\n",
    "score = sessions.mean()\n",
    "print(f\"Your score: {score}\")\n",
    "\n",
    "assert score >= 500, \"Needs more training?\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, monitor=True,\n",
    "                             clip_reward=False, summaries=False, episodic_life=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# record sessions\n",
    "sessions = evaluate(env_monitor, policy, n_games=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([980., 650., 445.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rewards for recorded games\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
